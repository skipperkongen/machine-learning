{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice's adventures in wonderla ...\n",
      "Total Characters:  144431\n",
      "Total distinct:  45\n",
      "Total Patterns:  144331\n",
      "Shape X: (144331, 100, 45)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create data + model\n",
    "\"\"\"\n",
    "\n",
    "with open('alice.txt') as fin:\n",
    "    raw_text = fin.read().lower()\n",
    "print (raw_text[0:30], '...')\n",
    "distinct_chars = sorted(list(set(raw_text)))\n",
    "n_chars = len(raw_text)\n",
    "n_distinct = len(distinct_chars)\n",
    "print (\"Total Characters: \", n_chars)\n",
    "print (\"Total distinct: \", n_distinct)\n",
    "\n",
    "int_to_char = dict([(i, c) for i, c in enumerate(distinct_chars)])\n",
    "char_to_oh = dict([(c, np.identity(n_distinct)[i: i+1][0]) for i, c in enumerate(distinct_chars)])\n",
    "\n",
    "window_size = 100\n",
    "data_X = []\n",
    "data_y = []\n",
    "for i in range(0, n_chars - window_size, 1): \n",
    "    seq_in = [char_to_oh[c] for c in raw_text[i: i + window_size]]\n",
    "    seq_out = char_to_oh[raw_text[i+window_size]]\n",
    "    data_X.append(seq_in)\n",
    "    data_y.append(seq_out)\n",
    "    \n",
    "n_patterns = len(data_X)\n",
    "print (\"Total Patterns: \", n_patterns)\n",
    "\n",
    "# Use one-hot encoded\n",
    "X = np.reshape(data_X, (n_patterns, window_size, n_distinct))\n",
    "y = np.reshape(data_y, (n_patterns, n_distinct))\n",
    "\n",
    "print('Shape X:', X.shape)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Training\n",
    "\"\"\"\n",
    "\n",
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "model.fit(X, y, epochs=200, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice.ipynb\r\n",
      "README.md\r\n",
      "alice.txt\r\n",
      "long_short_term_memory_networks_with_python_mini_course.pdf\r\n",
      "requirements.txt\r\n",
      "weights-improvement-00-2.3455.hdf5\r\n",
      "weights-improvement-01-1.8025.hdf5\r\n",
      "weights-improvement-02-1.6004.hdf5\r\n",
      "weights-improvement-03-1.4707.hdf5\r\n",
      "weights-improvement-04-1.3777.hdf5\r\n",
      "weights-improvement-05-1.3072.hdf5\r\n",
      "weights-improvement-06-1.2478.hdf5\r\n",
      "weights-improvement-07-1.1951.hdf5\r\n",
      "weights-improvement-08-1.1507.hdf5\r\n",
      "weights-improvement-09-1.1090.hdf5\r\n",
      "weights-improvement-10-1.0702.hdf5\r\n",
      "weights-improvement-11-1.0367.hdf5\r\n",
      "weights-improvement-12-1.0051.hdf5\r\n",
      "weights-improvement-13-0.9761.hdf5\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Seed:\n",
      "\" even if i fell off the top\n",
      "of the house!' (which was very likely true.)\n",
      "\n",
      "down, down, down. would the \"\n",
      "Generated text:\n",
      " way of the door of the time as she could see anything to see it as she could see anything that she was so much at the mouse was out of the words 'it's a great deal to think i can take the time.'\n",
      "\n",
      "'i don't know what i say thing i can't seem to be a grin, and then the same thing the sea--'\n",
      "\n",
      "'i didn't know what i say thing i can great down on the dance?\n",
      " will you, won't you, will you, won't you, will you, won't you, will you, won't you, will you, won't you, will you, won't you, will you, won't you, will you, won't you, will you, won't you, will you, won't you, will you, won't you, will you, won't you, will you, won't you, will you, won't you, will you, won't you, will you, won't you, will you, won't you, will you, won't you, will you, won't you, will you, won't you, will you, won't you, will you, won't you, will you, won't you, will you, won't you, will you, won't you, will you, won't you, will you, won't you, will you, won't you, will you, won't you, will you, won't you, will you, won't\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Use model\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import random\n",
    "# load the network weights\n",
    "filename = \"weights-improvement-13-0.9761.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "def oh_to_char(oh):\n",
    "    return int_to_char[np.argmax(oh)]\n",
    "\n",
    "# pick a random seed\n",
    "\n",
    "pattern = [oh for oh in random.sample(data_X, 1)[0]]\n",
    "print ('---')\n",
    "print ('Seed:')\n",
    "print ('\"', ''.join([oh_to_char(oh) for oh in pattern]), '\"')\n",
    "\n",
    "print('Generated text:')\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "    X_next = np.reshape(pattern, (1, window_size, n_distinct))\n",
    "    prediction = model.predict(X_next, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    predicted_char = int_to_char[index]\n",
    "    sys.stdout.write(predicted_char)\n",
    "    padding = char_to_oh[predicted_char]\n",
    "    pattern.append(padding)\n",
    "    pattern = pattern[1:]\n",
    "print ()\n",
    "print ('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
